{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75bd5c4-0908-4283-844d-3a0767b5b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03791e91-aa38-4194-8099-55d2799012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
      "Download and unzip complete.\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import os\n",
    "\n",
    "# Set Kaggle API key directory (assuming kaggle.json is in the same folder as this notebook)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "\n",
    "# Download the dataset\n",
    "kaggle.api.dataset_download_files('adityajn105/flickr8k', path='flickr8k_data', unzip=True)\n",
    "\n",
    "print(\"Download and unzip complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86eac112-dd81-46af-afa3-983edb813e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_FILE = 'flickr8k_data/captions.txt'\n",
    "IMAGE_DIR = 'flickr8k_data/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e155e6e-5043-482a-a629-1e7c5ec79fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read captions file\n",
    "def load_captions(filename):\n",
    "    captions = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            tokens = line.split('\\t')\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            image_id = tokens[0].split('#')[0]\n",
    "            caption = tokens[1]\n",
    "            captions.append([image_id, caption])\n",
    "    df = pd.DataFrame(captions, columns=['image', 'caption'])\n",
    "    return df\n",
    "\n",
    "captions_df = pd.read_csv(CAPTIONS_FILE)\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d2ba0f-8ac4-43d1-a2d5-da2968c03301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 40456\n",
      "First 5 lines:\n",
      "image,caption\n",
      "\n",
      "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "\n",
      "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
      "\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
      "\n",
      "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(CAPTIONS_FILE, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(\"First 5 lines:\")\n",
    "for line in lines[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38152e83-e088-4d8c-8968-237865de9e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq child in pink dress is climbing up se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq girl going into wooden building endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl climbing into wooden play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl climbing the stairs to he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq little girl in pink dress going into ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  startseq child in pink dress is climbing up se...  \n",
       "1    startseq girl going into wooden building endseq  \n",
       "2  startseq little girl climbing into wooden play...  \n",
       "3  startseq little girl climbing the stairs to he...  \n",
       "4  startseq little girl in pink dress going into ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    data['caption'] = data['caption'].apply(lambda x: ' '.join([word for word in x.split() if len(word)>1]))\n",
    "    data['caption'] = data['caption'].apply(lambda x: 'startseq ' + x + ' endseq')\n",
    "    return data\n",
    "\n",
    "captions_df = text_preprocessing(captions_df)\n",
    "captions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cfaf33d-711d-439b-83bc-e8b268b2d9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8811\n",
      "Max caption length: 34\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer: fit on captions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_df['caption'].values)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Maximum caption length\n",
    "max_length = max(len(c.split()) for c in captions_df['caption'])\n",
    "print(\"Max caption length:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b349668-f43c-49b9-b5fa-66ed5d0aeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InceptionV3 model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "inception_model = InceptionV3(weights='imagenet')\n",
    "model_new = Model(inception_model.input, inception_model.layers[-2].output)\n",
    "\n",
    "# Image preprocessing\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(299, 299))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Feature extraction\n",
    "def encode_image(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    feature_vector = model_new.predict(img, verbose=0)\n",
    "    feature_vector = np.reshape(feature_vector, feature_vector.shape[1])\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91dd4675-26a8-4785-ac48-b476580ce08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "feature_vector = encode_image(IMAGE_DIR + \"/1000268201_693b08cb0e.jpg\")\n",
    "print(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecce735a-20a1-4c32-a68e-9f46d86ea29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, tokenizer, max_length, batch_size, image_path, vocab_size):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.image_path = image_path\n",
    "        self.vocab_size = vocab_size\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indexes]\n",
    "        return self.__data_generation(batch_df)\n",
    "    \n",
    "    def __data_generation(self, batch_df):\n",
    "        X1, X2, y = [], [], []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            photo = encode_image(os.path.join(self.image_path, row['image']))\n",
    "            seq = self.tokenizer.texts_to_sequences([row['caption']])[0]\n",
    "            \n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                \n",
    "                X1.append(photo)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "        \n",
    "        return [np.array(X1), np.array(X2)], np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cbaebd3-cd7f-4d44-86b6-dc28c057fda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,255,616</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8811</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,264,427</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,255,616\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8811\u001b[0m)              │       \u001b[38;5;34m2,264,427\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,635,691</span> (21.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,635,691\u001b[0m (21.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,635,691</span> (21.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,635,691\u001b[0m (21.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Image feature input\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Sequence input (captions)\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# Combine\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# Define full model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6859339e-ec61-40ba-a47f-c7432f9173a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cee8b80d5c7449ea6ac0aee5f56a11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 8091 images.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Precompute image features for all unique images\n",
    "image_features = {}\n",
    "\n",
    "unique_images = captions_df['image'].unique()\n",
    "\n",
    "for img_name in tqdm(unique_images):\n",
    "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "    image_features[img_name] = encode_image(img_path)\n",
    "\n",
    "print(f\"Extracted features for {len(image_features)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd2740f-7747-4b36-bf89-5a04ef9b9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_features.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('image_features.pkl', 'wb') as f:\n",
    "    pickle.dump(image_features, f)\n",
    "\n",
    "print(\"Saved image_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d182aa-11cc-42c2-8250-5b34f868604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, tokenizer, max_length, batch_size, image_features, vocab_size, shuffle=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.image_features = image_features\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indexes]\n",
    "        return self.__data_generation(batch_df)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_df):\n",
    "        X1, X2, y = [], [], []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            img_feature = self.image_features[row['image']]\n",
    "            seq = self.tokenizer.texts_to_sequences([row['caption']])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "\n",
    "                X1.append(img_feature)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "        return [np.array(X1), np.array(X2)], np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47d6d756-1884-47e4-8eeb-4539e4a5f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "batch_size = 64\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    captions_df, tokenizer, max_length, batch_size, image_features, vocab_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4608aeac-841e-420a-b6f3-da19dbb4c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image_features.pkl \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save image_features\n",
    "with open('image_features.pkl', 'wb') as f:\n",
    "    pickle.dump(image_features, f)\n",
    "\n",
    "print(\"Saved image_features.pkl \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6ea8d-d848-435a-a78d-9776fd188637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load image_features\n",
    "with open('image_features.pkl', 'rb') as f:\n",
    "    image_features = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(image_features)} image features \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baaaa8f-d9df-4ccc-937e-e32f2dbf46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 10\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "model.fit(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
